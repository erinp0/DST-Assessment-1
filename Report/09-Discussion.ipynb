{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b8d0e5",
   "metadata": {},
   "source": [
    "# Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acdd1c4",
   "metadata": {},
   "source": [
    "## 1. Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d6060",
   "metadata": {},
   "source": [
    "Given the highly imbalanced nature of our dataset, we decided against using accuracy as a performance metric. This is because we could achieve a really high accuracy score by simply predicting that all observations belong to the majority class (no heart disease). However, seeing as we care more about predicting the heart disease class (1) correctly, this would not be a successful model.\n",
    "\n",
    "Furthermore, for the context of our problem, we have been provided with a budget of £X million for heart disease screening and we must decide what threshold (what probability our model predicts a patient is positive for heart disease) is appropriate given the estimated cost of heart disease screening (e.g. a cholesterol test) is £Y per patient. This lends itself to caring more about minimising false negatives and focusing on the positive class as we do not want to misidentify someone who has heart disease as being negative for it.\n",
    "\n",
    "We initially investigate the ROC curve which shows you sensitivity and specificity at all possible thresholds. So if you find a point that represents the right tradeoff, you can choose the threshold that goes with that point on the curve. However, some literature argues that models trained on imbalanced datasets may seem to perform well when you look at an ROC curve, but when looking at the precision recall curve they do not perform well at all [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/). \n",
    "\n",
    "When evaluating just by the ROC curve, the linear regression model (baseline) was almost a winner at all thresholds as most of the curve lies above all the other model curves. It therefore also had the highest AUC score. [The Relationship Between Precision-Recall and ROC Curves, 2006](https://www.biostat.wisc.edu/~page/rocpr.pdf) states that if a curve dominates in ROC, it also dominates in PR, which was also the case when we plotted it. This surprised us as the literature we read indicated that for binary classification, linear regression models generally perform badly when the target variable is binary [Why Linear Regression is not suitable for classification?, 2021](https://medium.com/analytics-vidhya/why-linear-regression-is-not-suitable-for-classification-cd724dd61cb8#:~:text=There%20are%20two%20things%20that,new%20data%20points%20are%20added.). Due to the high dimensionality and imbalance of the data, we expected the boosted decision tree to outperform.\n",
    "\n",
    "This propmpted us to investigate the results further, looking more closely at the recall and precision individually, in which we found some more expected results. The linear regression model was somewhat cheating with its precision scores, as there were only probabilities under 0.5, giving it apparent \"perfect\" recall past this threshold. In reality this is meaningless, as it is simply prediciting all negatives. This skews its results in the ROC and Precision-Recall curves.\n",
    "\n",
    "In conclusion, we decided to evaluate our models on their Recall-Threshold and Precision-Recall curves. The aim of this is to favour minimising false negatives without producing too many false positives to handle. Since we have not defined what weighting this favour has, what remains will describe what we would proceed to do if we had.\n",
    "\n",
    "The F-measure is defined to be the harmonic mean of precision and recall. This means it is a measure of models, giving equal weight to the precision and recall. By our results from the Precision-Recall curve, plotting the F-measure against threshold would give the BDT as the winner over KNN. [A Gentle Introduction to the Fbeta-Measure for Machine Learning](https://machinelearningmastery.com/fbeta-measure-for-machine-learning/) introdcues us to a variation on the F-Measure, the Fbeta-Measure. The Fbeta-Measure is the weighted harmonic mean of precision and recall, i.e. a beta of 2 gives double the weight to precision. Therefore, by increasing beta, and plotting the Fbeta-Measure against threshold, we will eventually approach the result of the Recall-Threshold curves. Thus, beta can give us the key to deciding our winner out of KNN and BDT.\n",
    "\n",
    "We hypothesise that the reason logistic regression performed worse is because it was trained on a downsmapled training set (due to memory restrictions), where it did not get to see all of the positive cases.\n",
    "\n",
    "We were planning to investigate a stacking model, where we feed the probabilities outputted by each of the models to another stacked model as features and see if this model performed better. However, as the linear regression model dominated every other model, this would not provide increased performance. We were also unsure whether the predictions made by the models or the errors in predictions made by the models were uncorrelated, hence stacking might not have been appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690ea6e",
   "metadata": {},
   "source": [
    "## 2. Boosted Decision Tree (imputed vs missing data training sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69a688",
   "metadata": {},
   "source": [
    "In the Boosted Decision Tree model development, two models and corresponding sets of predictions were produced. The aim of this was to see if the information from the missing data was useful to the model. In the performance evaluation, it was decided that the missing data BDT model did in fact perform better. However, due to time constraints of the project, hyperparameter tuning was only able to take place for the missing data BDT model, and therefore it is not possible to put the results down to one or the other of hyperparameter tuning or retention of the missing data.\n",
    "\n",
    "We can conclude that at least one of these approaches improved the model and a better result would be achieved out of it if the predictions were to be used in the real world, but more investihation would be required to know the true causation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc560c",
   "metadata": {},
   "source": [
    "## 3. KNN (equal ratio vs preserved ratio downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938218e",
   "metadata": {},
   "source": [
    "The model which was trained on the dataset with equal class ratios outperformed the model trained on the dataset with the preserved ratios. According to the ROC curve, it dominated at thresholds above 0.1.\n",
    "\n",
    "We also looked at the recall plotted against the thresholds. This curve highlighted that the equal ratio model outperformed when looking strictly at recall also, confirming our conclusion that the equal ratio model is better suited to our data.\n",
    "\n",
    "The training data with 50/50 class split (KNN Equal) involved downsampling by reducing the count of training samples falling under the majority class. The risk associated with doing this is that by removing the collected data, we tend to lose a lot of valuable information. \n",
    "As the train test split was done using stratified sampling, the class distributions in the training set and test set are equal. We would therefore expect the KNN model with the preserved class distribution to outperform. Our results are a little surprising, however, the model with equal class ratio was exposed to more positive cases so it could predict positive cases in the test set better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e92c8",
   "metadata": {},
   "source": [
    "## 4. Limitations of Our Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35481d9",
   "metadata": {},
   "source": [
    "We recognise that our project is not fully polished and there are limitations to the conclusions we have drawn.\n",
    "* Some of our models could not train on the full dataset (KNN & LogReg) hence we cannot predict how well they will generalise to the full population.\n",
    "* For the logistic regression model, there was a trade off between information loss and dimensionality reduction.\n",
    "* The selected KNN model used an approximate search algorithm (KD Tree) for time saving purposes, however as the Mahalanobis distance metric is incompatible with KD Trees, the Euclidean distance was used. This is a limitation as the Euclidean distance cannot detect high correlation between variables.\n",
    "* In the BDT models, the class imbalance weighting correction used was incorrect and due to the time restraints was not able to be rectified. This means that the results are probably worse than they could have been.\n",
    "* Within the logisitic regression model, it is not easy to interpret and is sensitive to towards the scale of\n",
    "* We allowed models to train on different datasets (dataset with missing values and the imputed dataset). This did not allow us to make fair comparisons.\n",
    "* Our imputation method only achieved a root mean squared error of 0.33, which considering the standardised data took values between 0 and 1, this is not very good. However, some columns had less than 5% missingness so the effect of the imputation may be negligble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afd0cc",
   "metadata": {},
   "source": [
    "## 5. Future works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7b805",
   "metadata": {},
   "source": [
    "Given the time constraints and difficulties we encountered, there are numerous avenues for further exploration with our data. Some of them are listed below:\n",
    "* Using Bluecrystal HPC would have massively reduced computing time, in particular for the boosted decision tree. Furthermore, for the KNN and Logistic Regression models, using Bluecrystal HPC could allow us to train on the whole dataset, rather than down sampling.\n",
    "* Implementing cross validation and hyperparameter tuning on all methods to try and improve performance.\n",
    "* Investigate different hyperparameter tuning methods such as Bayesian optimisation.\n",
    "* Investigate other imputation methods and compare their performance (RMSE).\n",
    "* For future renditions of the logisitic regression model we would like to perform PCA on the dataset before performing dimensionality reduction. Dimensionality reduction would be done by selecting the number of components using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb429d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
