{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89cc5f6",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3044b1",
   "metadata": {},
   "source": [
    "We need to first import the necessary packages and files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17301dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from numpy import exp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69aee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_training= pd.read_csv(\"C:/Users/Calvi/Documents/Data Science Toolbox/Assessment 1/data/TrainingDataImputation.csv\")\n",
    "heart_disease_test = pd.read_csv(\"https://raw.githubusercontent.com/erinp0/DST-Assessment-1/main/Erin%20Pollard/test_imputed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d759f3",
   "metadata": {},
   "source": [
    "We then separate the features and target variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e63e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(heart_disease_training.iloc[:, 0]).astype(int)[np.newaxis,:].T\n",
    "y_test = np.array(heart_disease_test.iloc[:, 0]).astype(int)[np.newaxis,:].T\n",
    "del heart_disease_training['HeartDiseaseorAttack']\n",
    "del heart_disease_test['HeartDiseaseorAttack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe8db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = heart_disease_training.to_numpy()\n",
    "x_test = heart_disease_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae00625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_function(data):\n",
    "    mean = data.mean(axis=1)\n",
    "    std = data.std(axis=1)\n",
    "    for i in range(data.shape[1]):\n",
    "        data[i] -= mean[i]\n",
    "        data[i] /= std[i]\n",
    "    return data,mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf966264",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = center_function(x_train)\n",
    "x_train_centered = cf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd95c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardised Training Data:\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  1.392049  0.099506  1.392049 -0.495064  0.099506  0.099506 -0.331341   \n",
      "1  0.827005  0.827005  0.827005 -1.354394  0.827005 -0.731137 -1.250518   \n",
      "2  1.013108  1.013108  1.013108 -1.047109  1.013108 -0.398000 -0.868369   \n",
      "3 -0.099784  1.527512 -1.727079 -0.718156  1.527512 -0.099784 -0.642215   \n",
      "4  0.693649  0.693649  0.693649 -1.398234  0.693649  0.693649 -1.243280   \n",
      "\n",
      "         7         8         9   ...        11        12        13        14  \\\n",
      "0 -1.193037 -1.193037 -1.193037  ...  1.392049  0.099506  0.530354 -1.109647   \n",
      "1 -0.731137  0.827005  0.827005  ...  0.827005 -0.731137  0.827005  0.827005   \n",
      "2 -0.398000  1.013108  1.013108  ...  1.013108 -1.809108  0.072370 -1.718068   \n",
      "3 -0.099784 -0.099784 -0.099784  ...  1.527512 -0.099784  0.442648 -1.622092   \n",
      "4  0.693649  0.693649  0.693649  ...  0.693649  0.693649  0.693649 -2.211744   \n",
      "\n",
      "         15        16        17        18        19        20  \n",
      "0 -1.109647  1.392049 -1.193037  1.392049  1.022751 -1.193037  \n",
      "1  0.827005  0.827005 -2.289279 -0.063362  0.381821 -1.596771  \n",
      "2 -1.718068 -0.398000  1.013108  1.013108 -0.196413 -0.241210  \n",
      "3 -1.727079 -0.099784  1.527512  1.295041 -0.332254 -0.280594  \n",
      "4 -2.211744  0.693649  0.693649 -0.136463 -0.136463 -0.920458  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Standardised Training Data:\")\n",
    "print(pd.DataFrame(x_train_centered).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52434620",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 99 # This is the amount of explained variance we desire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2565d6",
   "metadata": {},
   "source": [
    "The function below retrives and orders the eigenvectors and values of the covariance matrix of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e487720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_eigen(data):\n",
    "    hd_scaled = StandardScaler().fit_transform(data)\n",
    "    features = hd_scaled.T\n",
    "    cov_matrix = np.cov(features)\n",
    "    values, vectors = np.linalg.eig(cov_matrix)\n",
    "    idx = values.argsort()[::-1]\n",
    "    vectors = vectors.T[:,idx]\n",
    "    return values,vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac8091",
   "metadata": {},
   "source": [
    "The pcafeatures function calculates the amount of components to have a certain amount of explained variance and outputs that,the explained variances and a list of the principle component names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480df06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcafeatures(data,percentage):\n",
    "    hd_scaled = StandardScaler().fit_transform(data)\n",
    "    features = hd_scaled.T\n",
    "    cov_matrix = np.cov(features)\n",
    "    values, vectors = np.linalg.eig(cov_matrix)\n",
    "    explained_variances = []\n",
    "    a = 0\n",
    "    num = 0\n",
    "    for i in range(len(values)):\n",
    "        explained_variances.append(values[i] / np.sum(values))\n",
    "    for i in range(len(explained_variances)):\n",
    "        while a < percentage:\n",
    "            a += explained_variances[i]*100\n",
    "            num += 1\n",
    "    pc_feat = ['PC{}'.format(i+1) for i in range(num)]\n",
    "    return pc_feat,num,explained_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fed4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking explained variance:\n",
      "[0.1348172518376701, 0.13032279301032065, 0.0955261856838543, 0.05931948440589512, 0.05573780236659315, 0.050289472213804365, 0.04707748013447749, 0.04502433250643572, 0.04147180060574487, 0.03935159134585836, 0.018428583013388018, 0.01968953915778834, 0.0224107095867694, 0.023331563422669396, 0.02507554363585574, 0.02679305099499639, 0.036237484854748536, 0.034725178890560066, 0.03356557452555949, 0.030312543120407234, 0.030492034686603404]\n",
      "Sum of explained variance is 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking explained variance:\")\n",
    "print(str(pcafeatures(x_train_centered,percentage)[2]))\n",
    "a = np.sum(pcafeatures(x_train_centered,percentage)[2])\n",
    "print(\"Sum of explained variance is\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5abde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_comps = pcafeatures(x_train_centered,percentage)[1]\n",
    "cov_evecs = cov_eigen(x_train_centered)[1][:,:num_of_comps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Covariance Eigenvectors:\")\n",
    "print(pd.DataFrame(cov_evecs).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca = np.dot(x_train_centered,cov_evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8e181",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(x_train_pca[:5,:], columns = pcafeatures(x_train_centered,percentage)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752f592",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Parameter initialize and sigmoid function\n",
    "\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_head = 1 / (1+exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658c29b",
   "metadata": {},
   "source": [
    "The sigmoid function is used for deciding the probability. It's clear from the graph below that the the higher the input it, the closer it tends to 1 and the lower it is, the closer it tends to 0. As we are working with binary classification, the only outcomes we desire are 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719eda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-10,10,20)\n",
    "plt.plot(x_axis,sigmoid(x_axis))\n",
    "plt.axhline(y=1,color = 'r',linestyle = '-')\n",
    "plt.axhline(y=0,color = 'r',linestyle = '-')\n",
    "plt.axhline(y=0.5,color = 'b', linestyle = ':')\n",
    "plt.axvline(x=0,color = 'r', linestyle = ':')\n",
    "plt.title(\"Sigmoid Function\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ec070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loss function\n",
    "\n",
    "def logloss(y_hat,y):\n",
    "    a = np.log(y_hat)\n",
    "    b = np.log(1-y_hat)\n",
    "    loss = (-1/x_train.shape[0]) * np.sum(y*a + (1-y)*b)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Forward and Backward Propagation\n",
    "\n",
    "def forward_backward_propagation(w,b,x_train,y_hat):\n",
    "    \n",
    "    z = np.dot(x_train,w) + b\n",
    "    y_hat = sigmoid(z)\n",
    "    cost = logloss(y_hat,y_train)\n",
    "    \n",
    "    #backward propogation\n",
    "    derivative_weight = np.dot((y_hat-y_train).T,x_train)/x_train.shape[0]\n",
    "    derivative_bias = np.sum(y_hat-y_train)/x_train.shape[0]\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc303eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Updating (Learning) Parameters\n",
    "    \n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters is number_of_iteration times\n",
    "    for i in range(number_of_iteration):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * (gradients[\"derivative_weight\"].T)\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 50 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost)) #if section defined to print our cost values in every 10 iteration. We do not need to do that. It's optional.\n",
    "     # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(range(len(cost_list)),cost_list)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Prediction\n",
    "\n",
    "def predict(w,b,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(x_test,w)+b)\n",
    "    Y_prediction = np.zeros((x_test.shape[0],1))\n",
    "    # if z is bigger than 0.5, our prediction is one means has diabete (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is zero means does not have diabete (y_head=0),\n",
    "    for i in range(len(z)):\n",
    "        if z[i]< 0.5:\n",
    "            Y_prediction[i] = 0\n",
    "        else:\n",
    "            Y_prediction[i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "#predict(parameters[\"weight\"],parameters[\"bias\"],x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # initialise\n",
    "    dimension =  x_train.shape[1]\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    \n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    \n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf75503",
   "metadata": {},
   "source": [
    "The probabilities funtion outputs the probabilities of each observable testing positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities(x_train, x_test, y_train, learning_rate,number_of_iteration):\n",
    "    # initialise\n",
    "    dimension =  x_train.shape[1]\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    # updating(learning) parameters is number_of_iteration times\n",
    "    for i in range(number_of_iteration):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        # lets update\n",
    "        w = w - learning_rate * (gradients[\"derivative_weight\"].T)\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        z = sigmoid(np.dot(x_test,w)+b)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511b6e8",
   "metadata": {},
   "source": [
    "We need to prepare our test dataset before predicting the outcome. We need to project the test data into the same space as the training. So we follow the standardising method but use the mean and standard deviation from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x_test.shape[1]):\n",
    "    x_test[:,i] -= cf[1][i]     \n",
    "    x_test[:,i] /= cf[2][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pca = np.dot(x_test,cov_evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b822d",
   "metadata": {},
   "source": [
    "# With and without PCA Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f464db",
   "metadata": {},
   "source": [
    "## Without PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 2, num_iterations = 235)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA, execution time of Logistic Regression is: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3c786",
   "metadata": {},
   "source": [
    "## With PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80536a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "logistic_regression(x_train_pca, y_train, x_test_pca, y_test,learning_rate =4 , num_iterations = 246)\n",
    "end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abe89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA, execution time of Logistic Regression is: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb97a516",
   "metadata": {},
   "source": [
    "We can look at the probabilities of a case being at risk of heart disease or an attack using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = probabilities(x_train_pca, x_test_pca, y_train, learning_rate = 4,number_of_iteration = 246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = pd.DataFrame(y_hat, columns = ['Probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
